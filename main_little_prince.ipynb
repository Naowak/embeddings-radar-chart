{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1pNf54eW96N"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKcdwIbCQkSb",
        "outputId": "4e087d4a-1772-4f97-dd04-d539fe3f2ecb"
      },
      "outputs": [],
      "source": [
        "%pip install transformers plotly tqdm numpy torch torchvision torchaudio pandas nbformat scipy imageio kaleido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load model & dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPoskQM7dXC-",
        "outputId": "2d27d097-29ba-4c39-f806-20378cd5a363"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import numpy as np\n",
        "import scipy\n",
        "import torch\n",
        "import random\n",
        "import imageio\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import os\n",
        "import pandas as pd\n",
        "torch.no_grad()\n",
        "\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load texts and get all tokens from them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\n",
        "with open(\"./little_prince_data/little_prince.txt\", 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "unique_tokens = list(set(tokens))\n",
        "\n",
        "# Save the unique tokens\n",
        "with open(\"./little_prince_data/unique_tokens.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(unique_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoDCae63XNNH"
      },
      "source": [
        "# Get \"raw\" embeddings for each token in our texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okHZzGFwTMPH",
        "outputId": "4ca06550-30c9-4572-dee7-2b0c5c791792"
      },
      "outputs": [],
      "source": [
        "# If you have trouble to compute everything once, you can go step by step with begin & end\n",
        "\n",
        "# Load vocabulary\n",
        "with open(\"./little_prince_data/unique_tokens.txt\") as f:\n",
        "    vocab = f.read().split(\"\\n\")\n",
        "\n",
        "# Compute embeddings\n",
        "raw_emb = []\n",
        "for token in tqdm(vocab):\n",
        "    id = tokenizer.convert_tokens_to_ids(token)\n",
        "    tensor = torch.tensor([id]).unsqueeze(0)\n",
        "    emb = model(tensor).last_hidden_state[0]\n",
        "    raw_emb.append(emb)\n",
        "\n",
        "# Save embeddings\n",
        "raw_emb = torch.stack(raw_emb).reshape(-1, 768).detach().numpy()\n",
        "np.save(f\"./little_prince_data/raw_emb.npy\", raw_emb)\n",
        "#np.save(f\"raw_emb_{begin}-{end}.npy\", vocab_emb)\n",
        "\n",
        "# In case you want to concatenate the matrices from different runs\n",
        "# file_list = glob.glob(\"*.npy\")\n",
        "# matrices = [np.load(file) for file in file_list]\n",
        "# raw_emb = np.concatenate(matrices, axis=0)\n",
        "# np.save('raw_emb.npy', raw_emb)\n",
        "\n",
        "# Stats\n",
        "print(\"Moyennes:\", np.mean(raw_emb, axis=0)[:5])\n",
        "print(\"Écarts-types:\", np.std(raw_emb, axis=0)[:5])\n",
        "\n",
        "# Display one dimension of the embedding matrix\n",
        "fig = px.histogram(raw_emb[:, 100])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_-kpq6NqPM-"
      },
      "source": [
        "# Compute correlation and display it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEfRZ6ASqUtZ",
        "outputId": "9a3de68d-da26-4322-9604-9a0ac4d44df6"
      },
      "outputs": [],
      "source": [
        "# Load std embeddings\n",
        "raw_emb = np.load('./little_prince_data/raw_emb.npy')\n",
        "\n",
        "# Calculer la matrice de corrélation\n",
        "corr = np.corrcoef(raw_emb, rowvar=False)\n",
        "np.save('./little_prince_data/corr.npy', corr)\n",
        "\n",
        "# Display heatmap\n",
        "fig = px.imshow(corr)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def matrix_stats(matrix):\n",
        "    \"\"\"Retourne les statistiques de la matrice en excluant la diagonale.\"\"\"\n",
        "    values = matrix[~np.eye(matrix.shape[0],dtype=bool)]\n",
        "    return {\n",
        "        'min': np.min(values),\n",
        "        'max': np.max(values),\n",
        "        'mean': np.mean(values),\n",
        "        'median': np.median(values)\n",
        "    }\n",
        "\n",
        "print(\"Statistics:\", matrix_stats(corr))\n",
        "\n",
        "# Histogramme des valeurs de corrélation\n",
        "fig = go.Figure(data=go.Histogram(x=corr[~np.eye(corr.shape[0], dtype=bool)], nbinsx=50))\n",
        "fig.update_layout(title='Distribution des Valeurs de Corrélation', xaxis_title='Valeur de Corrélation', yaxis_title='Compte')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compute absolute correlation and display it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Charger la matrice de corrélation\n",
        "corr = np.load('./little_prince_data/corr.npy')\n",
        "\n",
        "# Calculer la matrice de corrélation absolue\n",
        "abs_corr = np.abs(corr)\n",
        "np.save('./little_prince_data/abs_corr.npy', corr)\n",
        "\n",
        "# Display heatmap\n",
        "fig = px.imshow(abs_corr)\n",
        "fig.show()\n",
        "\n",
        "# Histogramme des valeurs de corrélation absolue\n",
        "fig2 = go.Figure(data=go.Histogram(x=abs_corr[~np.eye(abs_corr.shape[0], dtype=bool)], nbinsx=50))\n",
        "fig2.update_layout(title='Distribution des Valeurs de Corrélation Absolue', xaxis_title='Valeur de Corrélation Absolue', yaxis_title='Compte')\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sort dimensions (using absolute correlation as a metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_CORR = 0.40\n",
        "\n",
        "def fitness(path, abs_corr_matrix):\n",
        "    N = len(path)\n",
        "    distance = 0\n",
        "    for ia, dim_a in enumerate(path):\n",
        "        for ib, dim_b in enumerate(path):\n",
        "            ang_a = ia / N * 2 * np.pi\n",
        "            ang_b = ib / N * 2 * np.pi\n",
        "            ang_dist = np.pi - abs(np.pi - abs(ang_a - ang_b)) # Distance angulaire bornée par Pi\n",
        "            expected_dist = max(np.pi - (1/MAX_CORR)*np.pi*abs_corr_matrix[dim_a, dim_b], 0) # Distance angulaire attendue\n",
        "            distance += (ang_dist - expected_dist) ** 2 # cout erreur quadratique\n",
        "    return distance\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "    \"\"\"Effectue un croisement en prenant un sous-chemin de parent1 et complète avec parent2.\"\"\"\n",
        "    # Sélectionne un sous-chemin aléatoire de parent1\n",
        "    start, end = random.sample(range(len(parent1)), 2)\n",
        "    \n",
        "    # Crée un enfant en copiant le sous-chemin de parent1\n",
        "    child = [-1]*len(parent1)\n",
        "    if start <= end:\n",
        "        for i in range(start, end + 1):\n",
        "            child[i] = parent1[i]\n",
        "    else:\n",
        "        for i in range(start, len(parent1)):\n",
        "            child[i] = parent1[i]\n",
        "        for i in range(0, end + 1):\n",
        "            child[i] = parent1[i]\n",
        "\n",
        "    # Complète l'enfant avec les éléments de parent2\n",
        "    pointer = 0\n",
        "    for i in range(len(child)):\n",
        "        if child[i] == -1:\n",
        "            while parent2[pointer] in child:\n",
        "                pointer += 1\n",
        "            child[i] = parent2[pointer]\n",
        "            pointer += 1\n",
        "\n",
        "    return child\n",
        "\n",
        "def mutate(path, mutation_rate, abs_corr_matrix, top_n):\n",
        "    \"\"\"Déplace quelques éléments du chemin à une autre position plus adaptée.\"\"\"\n",
        "    # Choose random elements to remove from the path\n",
        "    selected_elems = [elem for elem in path if random.random() < mutation_rate]\n",
        "    new_path = [elem for elem in path if elem not in selected_elems]\n",
        "\n",
        "    # Insert the removed elements at random best position\n",
        "    for elem in selected_elems:\n",
        "\n",
        "        # Compute the score of each position\n",
        "        scores = []\n",
        "        for i in range(len(new_path)):\n",
        "            prev = new_path[i]\n",
        "            next = new_path[(i+1)%len(new_path)]\n",
        "            scores += [abs_corr_matrix[prev, elem] + abs_corr_matrix[elem, next]]\n",
        "\n",
        "        # Select the top_n best positions\n",
        "        ranked_indices = np.argsort(scores)[-top_n:] # Index of the top_n positions\n",
        "        ranked_scores = np.sort(scores)[-top_n:] # Score of the top_n positions\n",
        "\n",
        "        # Normalize the scores to get a probability distribution\n",
        "        proba = ranked_scores / np.sum(ranked_scores)\n",
        "\n",
        "        # Choose a position according to the probability distribution\n",
        "        new_pos = np.random.choice(ranked_indices, p=proba)\n",
        "        new_path.insert(new_pos, elem)\n",
        "    \n",
        "    return new_path\n",
        "\n",
        "def genetic_algorithm(abs_corr_matrix, population_size, generations, mutation_rate, top_n_mutate, top_n_parents):\n",
        "    \"\"\"Effectue l'algorithme génétique pour le TSP.\"\"\"\n",
        "    \n",
        "    # Génère une population initiale de chemins aléatoires\n",
        "    population = [list(np.random.permutation(len(abs_corr_matrix))) for _ in range(population_size)]\n",
        "    best_distance = np.inf\n",
        "    best_path = None\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Évalue la fitness de chaque individu\n",
        "        distances = [fitness(path, abs_corr_matrix) for path in population]\n",
        "        ranked_indices = np.argsort(distances)\n",
        "        print(f\"Gen {generation}: {min(distances)}\")\n",
        "        #print('scores', distances)\n",
        "\n",
        "        # Sauvegarde le meilleur individu si meilleur que courant\n",
        "        if min(distances) < best_distance:\n",
        "            best_distance = min(distances)\n",
        "            best_path = population[np.argmin(distances)]\n",
        "\n",
        "        # Sélection des parents pour le croisement\n",
        "        parents = [population[i] for i in ranked_indices[:top_n_parents]]\n",
        "        #print('parents', parents)\n",
        "\n",
        "        # Création de la nouvelle population\n",
        "        new_population = [p for p in parents] # Copie les parents\n",
        "        while len(new_population) < population_size:\n",
        "            parent1, parent2 = random.sample(parents, 2)\n",
        "            child = crossover(parent1, parent2)\n",
        "            new_population.append(child)\n",
        "\n",
        "        # Appliquer la mutation\n",
        "        population = [mutate(path, mutation_rate, abs_corr_matrix, top_n_mutate) for path in new_population]\n",
        "\n",
        "    # Renvoie le meilleur chemin trouvé\n",
        "    return best_path, best_distance\n",
        "\n",
        "# Charger la matrice de corrélation absolue\n",
        "abs_corr = np.load('./little_prince_data/abs_corr.npy')\n",
        "\n",
        "# Appliquer l'algorithme génétique\n",
        "best_path, best_distance = genetic_algorithm(\n",
        "    abs_corr_matrix=abs_corr, \n",
        "    population_size=100, \n",
        "    generations=300, \n",
        "    mutation_rate=0.01, \n",
        "    top_n_mutate=5, \n",
        "    top_n_parents=10,\n",
        ")\n",
        "\n",
        "print(\"Meilleur chemin:\", best_path)\n",
        "print(\"Meilleur distance:\", best_distance)\n",
        "\n",
        "with open('./little_prince_data/path.txt', 'w') as f:\n",
        "    f.write(str(best_path) + '\\n')\n",
        "    f.write(str(best_distance))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp4veVDBW321"
      },
      "source": [
        "# Radar Chart (absolute correlation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Load and prepare data & filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load path\n",
        "with open('./little_prince_data/path.txt', 'r') as f:\n",
        "    path = eval(f.readline())\n",
        "\n",
        "# Load the correlation matrix\n",
        "corr = np.load('./little_prince_data/corr.npy')\n",
        "\n",
        "# Compute coef for each value in the path\n",
        "coefs = [1] # List that contains values {1, -1} for each value in the path\n",
        "current_coef = 1\n",
        "for i in range(len(path)):\n",
        "    current = path[i]\n",
        "    next = path[(i+1)%len(path)]\n",
        "    next_coef = int(np.sign(corr[current, next]))\n",
        "    coefs += [current_coef * next_coef]\n",
        "    current_coef = next_coef\n",
        "\n",
        "# Define filter\n",
        "def circular_convolution(signal, kernel):\n",
        "    len_kernel = len(kernel)\n",
        "    half_kernel = len_kernel // 2\n",
        "    extended_signal = np.concatenate([signal[-half_kernel:], signal, signal[:half_kernel]]) # Étendre le signal aux deux extrémités pour gérer le cas circulaire\n",
        "    result = np.convolve(extended_signal, kernel, mode='valid') # Appliquer la convolution standard\n",
        "    return result\n",
        "\n",
        "N = 32\n",
        "filter = [scipy.stats.norm.pdf(3*x/(N+1), 0, 1) for x in range(-N, N+1)]\n",
        "filter_norm = [x / sum(filter) for x in filter]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Display words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-order the dimensions with coefs and apply the filter\n",
        "def compute_word_shape(emb, path, coefs, filter):\n",
        "    sorted = [emb[dim] * coefs[i] for i, dim in enumerate(path)]\n",
        "    filtered = circular_convolution(np.array(sorted), filter)\n",
        "    return filtered\n",
        "\n",
        "# Compute the shape of some words\n",
        "words = ['kill', 'death']\n",
        "ids = [tokenizer.encode(word, add_special_tokens=False)[0] for word in words]\n",
        "embs = [model(torch.tensor([id]).unsqueeze(0)).last_hidden_state[0].detach().numpy() for id in ids]\n",
        "shapes = [compute_word_shape(emb[0], path, coefs, filter_norm) for emb in embs]\n",
        "\n",
        "# Display the shape of the words\n",
        "fig = go.Figure()\n",
        "for shape, word in zip(shapes, words):\n",
        "    fig.add_trace(go.Scatterpolar(r=shape, name=word, fill='toself', opacity=0.60))\n",
        "fig.update_layout({'polar': {\n",
        "    \"radialaxis\": {\"range\": [-0.8, 0.8], \"showticklabels\": False}, \n",
        "    \"angularaxis\": {\"showticklabels\": False},\n",
        "}})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Display word representation depending on context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-order the dimensions with coefs and apply the filter\n",
        "def compute_word_shape(emb, path, coefs, filter):\n",
        "    sorted = [emb[dim] * coefs[i] for i, dim in enumerate(path)]\n",
        "    filtered = circular_convolution(np.array(sorted), filter)\n",
        "    return filtered\n",
        "\n",
        "# Define sentence and word of interest\n",
        "sentence = \"It would have been better to come back at the same hour\"\n",
        "interest = \"better\"\n",
        "\n",
        "# Define context before & after\n",
        "words = sentence.lower().split()\n",
        "pos_interest = words.index(interest.lower())\n",
        "context_before = list(reversed([' '.join(words[i:pos_interest+1]) for i in range(pos_interest+1)]))\n",
        "context_after = [' '.join(words[pos_interest:i+1]) for i in range(pos_interest, len(words))]\n",
        "\n",
        "# Compute word interest's embedding for each context\n",
        "embs_before = []\n",
        "embs_after = []\n",
        "for cb in context_before:\n",
        "    ids = tokenizer.encode(cb)\n",
        "    tensor = torch.tensor(ids).unsqueeze(0)\n",
        "    emb = model(tensor).last_hidden_state[0][-2].tolist()\n",
        "    embs_before.append(emb)\n",
        "for ca in context_after:\n",
        "    ids = tokenizer.encode(ca)\n",
        "    tensor = torch.tensor(ids).unsqueeze(0)\n",
        "    emb = model(tensor).last_hidden_state[0][1].tolist()\n",
        "    embs_after.append(emb)\n",
        "\n",
        "# Create the figure\n",
        "max_len = max(len(embs_before), len(embs_after))\n",
        "specs = [[{\"type\": \"scatterpolar\"} for _ in range(max_len)] for _ in range(2)]\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=max_len, start_cell=\"top-left\", specs=specs, \n",
        ")\n",
        "\n",
        "# Display context before\n",
        "for i, eb in enumerate(embs_before):\n",
        "    shape = compute_word_shape(eb, path, coefs, filter_norm)\n",
        "    polar = go.Scatterpolar(r=shape, name=context_before[i], fill='toself', opacity=0.60, line=dict(color='blue'))\n",
        "    fig.add_trace(polar, row=1, col=i+1)\n",
        "for i, ea in enumerate(embs_after):\n",
        "    shape = compute_word_shape(ea, path, coefs, filter_norm)\n",
        "    polar = go.Scatterpolar(r=shape, name=context_after[i], fill='toself', opacity=0.60, line=dict(color='red'))\n",
        "    fig.add_trace(polar, row=2, col=i+1)\n",
        "\n",
        "\n",
        "# Ajout des annotations pour les titres en diagonale\n",
        "for i in range(max_len):\n",
        "    # Position pour les titres de la rangée \"Before\"\n",
        "    if i < len(context_before):\n",
        "        fig.add_annotation(\n",
        "            x=(i + 0.4) / max_len,  # Calculate the center position\n",
        "            y=0.95,                # Slightly above the top of the subplot\n",
        "            text=context_before[i],\n",
        "            showarrow=False,\n",
        "            xref=\"paper\",\n",
        "            yref=\"paper\",\n",
        "            xanchor=\"center\",\n",
        "            yanchor=\"bottom\",\n",
        "            align=\"left\",\n",
        "            font=dict(size=25),\n",
        "            textangle=-30\n",
        "        )\n",
        "\n",
        "    # Position pour les titres de la rangée \"After\"\n",
        "    if i < len(context_after):\n",
        "        fig.add_annotation(\n",
        "            x=(i + 0.4) / max_len,  # Calculate the center position\n",
        "            y=0.40,               # Slightly below the bottom of the subplot\n",
        "            text=context_after[i],\n",
        "            showarrow=False,\n",
        "            xref=\"paper\",\n",
        "            yref=\"paper\",\n",
        "            xanchor=\"center\",\n",
        "            yanchor=\"bottom\",\n",
        "            align=\"left\",\n",
        "            font=dict(size=25),\n",
        "            textangle=-30\n",
        "        )\n",
        "\n",
        "# Update layout to remove axis and show figure\n",
        "layout = {\n",
        "    \"showlegend\": False, \n",
        "    \"title_text\": f\"Evolution of the word \\\"{interest}\\\" in function of its context (before and after)\",\n",
        "    \"title_x\": 0.5, \n",
        "    \"title_font\": {\"size\": 40},\n",
        "    \"width\": 3000,\n",
        "    \"height\": 1600,\n",
        "    \"margin\": {'t': 250},\n",
        "}\n",
        "for i in range(1, 2*max_len+1):\n",
        "    layout[f\"polar{i}\"] = {\n",
        "        \"radialaxis\": {\"range\": [-0.3, 0.3], \"showticklabels\": False}, \n",
        "        \"angularaxis\": {\"showticklabels\": False},\n",
        "    } \n",
        "fig.update_layout(layout)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "embeddings-radar-chart",
      "language": "python",
      "name": "embeddings-radar-chart"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
